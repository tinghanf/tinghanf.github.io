<head>
<meta name="google-site-verification" content="RaPdlErOpwDKyAdw8zZKzv_ciBUPEzwJUrAxWl7MWHM" />
<body>

<center>
<h1> Ting-Han Fan </h1>
</center>
  
<table border="0" cellpadding="5" cellspacing="10">
<tbody><tr>

<td valign="middle">
<img src="fan-tinghan-01.jpg" height="200">
</td>

<td valign="middle">
<p>I am a machine learning engineer at <a href="https://shop.tiktok.com">TikTok E-commerce Recommendation</a>. 
Previously, I was a summer associate at <a href="https://www.gsam.com">Goldman Sachs Asset Management</a>, 
a research intern at <a href="https://www.siemens.com">Siemens</a> and <a href="https://www.microsoft.com">Microsoft</a>.
I received my M.A. and Ph.D. degree from <a href="https://ece.princeton.edu/">Electrical and Computer Engineering, Princeton University</a>, 
advised by Prof. <a href="https://ece.princeton.edu/people/peter-j-ramadge">Peter J. Ramadge</a>. 
Before Princeton, I received my B.S. degree from <a href="https://web.ee.ntu.edu.tw/">Electrical Engineering, National Taiwan University</a>.</p>

<p>email: tinghanfan at gmail dot com</p>

<p> Find me at <a href="https://scholar.google.com/citations?user=1mQ3kTEAAAAJ&hl=en">Google Scholar</a> 
and <a href="https://www.linkedin.com/in/ting-han-fan">LinkedIn</a>. </p>
</td>

</tr></tbody>
</table>


<h4>Publications:</h4>
<ul>
    <li>
    Regular Language Reasoning
    <ul>
        <li> <a href="https://arxiv.org/abs/2305.03796">Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>Findings of EMNLP 2023.</strong></li>
        <li> <a href="https://arxiv.org/abs/2309.07412">Advancing Regular Language Reasoning in Linear Recurrent Neural Networks</a>, Ting-Han Fan*, Ta-Chung Chi*, Alexander I. Rudnicky, <strong>arXiv:2309.07412.</strong></li>
    </ul>
    </li>

    <li>
    Positional Embeddings for Transformer Language Models 
    <ul>
        <li> <a href="https://aclanthology.org/2023.acl-long.756">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023. <a style="color:red">(Outstanding Paper Award)</a></strong></li>
        <li> <a href="https://aclanthology.org/2023.acl-short.102">Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings</a>, Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023. <a style="color:blue">(Award Nomination, top 1.6% of submission)</a></strong></li>
        <li> <a href="https://papers.nips.cc/paper_files/paper/2022/hash/37a413841a614b5414b333585e7613b8-Abstract-Conference.html">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation</a>, Ta-Chung Chi*, Ting-Han Fan*, Peter J. Ramadge, Alexander I. Rudnicky, <strong>NeurIPS 2022.</strong> <a href="https://github.com/chijames/KERPLE">[code]</a></li>
    </ul>
    </li>
    
    <li>
    Reparameterization for Discrete Deep Generative Models
    <ul>
        <li> <a href="https://proceedings.mlr.press/v162/fan22a.html">Training Discrete Deep Generative Models via Gapped Straight-Through Estimator</a>, Ting-Han Fan*, Ta-Chung Chi*, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ICML 2022.</strong> <a href="https://github.com/chijames/GST">[code]</a> <a href="https://slideslive.com/38984237/training-discrete-deep-generative-models-via-gapped-straightthrough-estimator">[SlidesLive]</a></li>
    </ul>
    </li>
    
    <li>
    Reinforcement Learning for Power Distribution System Controls
    <ul>
        <li> <a href="https://proceedings.mlr.press/v168/fan22a.html">PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems</a>, Ting-Han Fan, Xian Yeow Lee, Yubo Wang, <strong>L4DC 2022.</strong> <a href="https://github.com/siemens/powergym">[code]</a></li>
        <li> <a href="https://ieeexplore.ieee.org/document/9867395">Soft Actor-critic With Integer Actions</a>, Ting-Han Fan, Yubo Wang, <strong>ACC 2022.</strong></li>
    </ul>
    </li>
    
    <li>
    Reinforcement Learning
    <ul>
        <li> <a href="https://proceedings.mlr.press/v130/fan21a.html">A Contraction Approach to Model-based Reinforcement Learning</a>, Ting-Han Fan, Peter J. Ramadge, <strong>AISTATS 2021.</strong> <a href="https://slideslive.com/38952954/a-contraction-approach-to-modelbased-reinforcement-learning">[SildesLive]</a></li>
    </ul>
    </li>
</ul>

<h4>Academic Activities:</h4>
<ul>
    <li> Paper Review: AISTATS 2021, L4DC 2023, ICML 2022-2023, NeurIPS 2022-2023, ICLR 2024.
    </li>
    
    <li> Teaching Assistant at Princeton University.
    <ul>
        <li>ECE 435/535: Machine Learning and Pattern Recognition, <em>Fall 2019, Fall 2020, Fall 2022 <strong>(head TA)</strong></em></li>
        <li>EGR 154: Foundations of Engineering: Linear Systems, <em>Spring 2022</em></li>
        <li>COS 302: Mathematics for Numerical Computing and Machine Learning, <em>Fall 2021 <strong>(head TA)</strong></em></li>
        <li>SML 310: Research Projects in Data Science, <em>Spring 2021 <strong>(head TA)</strong></em></li>
        <li>ECE 201: Information Signals, <em>Spring 2020, Spring 2023</em></li>
    </ul>
    </li>
    
</ul>

</body>
</head>
