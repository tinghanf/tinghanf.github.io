<html>
<body>

<center>
<h1> Ting-Han Fan </h1>
</center>
  
<table border="0" cellpadding="5" cellspacing="10">
<tbody><tr>

<td valign="middle">
<img src="fan-tinghan-01.jpg" height="150">
</td>

<td valign="middle">
<p>I am an incoming machine learning engineer at <a href="https://shop.tiktok.com">TikTok E-commerce Recommendation</a>. 
Previously, I was a summer associate at <a href="https://www.gsam.com">Goldman Sachs Asset Management</a>, 
a research intern at <a href="https://www.siemens.com">Siemens</a> and <a href="https://www.microsoft.com">Microsoft</a>.
I received my Ph.D. degree from <a href="https://ece.princeton.edu/">Electrical and Computer Engineering, Princeton University</a>, 
advised by Prof. <a href="https://ece.princeton.edu/people/peter-j-ramadge">Peter J. Ramadge</a>. 
Before Princeton, I received my Bachelor degree from <a href="https://web.ee.ntu.edu.tw/">Electrical Engineering, National Taiwan University</a>.</p>

<p>email: tinghanf at alumni dot princeton dot edu</p>

<p> Find me at <a href="https://scholar.google.com/citations?user=1mQ3kTEAAAAJ&hl=en">Google scholar</a> 
and <a href="https://www.linkedin.com/in/ting-han-fan-70716a143/">LinkedIn</a>. </p>
</td>

</tr></tbody>
</table>


<h4>Publications:</h4>
<ul>
    <li>
    Transformers for Regular Language Reasoning
    <ul>
        <li> <a href="https://arxiv.org/abs/2305.03796">Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>under review</strong></li>
    </ul>
    </li>

    <li>
    Positional Embeddings for Transformer Language Models 
    <ul>
        <li> <a href="https://arxiv.org/abs/2305.13571">Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings</a>, Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023</strong></li>
        <li> <a href="https://arxiv.org/abs/2212.10356">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023</strong></li>
        <li> <a href="https://arxiv.org/abs/2205.09921">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation</a>, Ta-Chung Chi*, Ting-Han Fan*, Peter J. Ramadge, Alexander I. Rudnicky, <strong>NeurIPS 2022</strong>. <a href="https://github.com/chijames/KERPLE">[code]</a></li>
    </ul>
    </li>
    
    <li>
    Reparameterization for Discrete Deep Generative Models
    <ul>
        <li> <a href="https://arxiv.org/abs/2206.07235">Training Discrete Deep Generative Models via Gapped Straight-Through Estimator</a>, Ting-Han Fan*, Ta-Chung Chi*, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ICML 2022</strong>. <a href="https://github.com/chijames/GST">[code]</a> <a href="https://slideslive.com/38984237/training-discrete-deep-generative-models-via-gapped-straightthrough-estimator">[SlidesLive]</a></li>
    </ul>
    </li>
    
    <li>
    Reinforcement Learning for Power Distribution System Controls
    <ul>
        <li> <a href="https://arxiv.org/abs/2109.03970">PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems</a>, Ting-Han Fan, Xian Yeow Lee, Yubo Wang, <strong>L4DC 2022</strong>. <a href="https://github.com/siemens/powergym">[code]</a></li>
        <li> <a href="https://arxiv.org/abs/2109.08512">Soft Actor-critic With Integer Actions</a>, Ting-Han Fan, Yubo Wang, <strong>ACC 2022</strong></li>
    </ul>
    </li>
    
    <li>
    Reinforcement Learning
    <ul>
        <li> <a href="https://arxiv.org/abs/2009.08586">A Contraction Approach to Model-based Reinforcement Learning</a>, Ting-Han Fan, Peter J. Ramadge, <strong>AISTATS 2021</strong>. <a href="https://slideslive.com/38952954/a-contraction-approach-to-modelbased-reinforcement-learning">[SildesLive]</a></li>
    </ul>
    </li>
</ul>

<h4>Academic Activities:</h4>
<ul>
    <li> I am/was a reviewer for the following machine learning conferences: AISTATS 2021, ICML 2022, NeurIPS 2022, L4DC 2023, ICML 2023, NeurIPS 2023.
    </li>
    
    <li> I was a teaching assistant for the following courses at Princeton University.
    <ul>
        <li>ECE 435/535: Machine Learning and Pattern Recognition, <em>Fall 2019, Fall 2020, Fall 2022<strong>(head TA)</strong></em></li>
        <li>EGR 154: Foundations of Engineering: Linear Systems, <em>Spring 2022</em></li>
        <li>COS 302: Mathematics for Numerical Computing and Machine Learning, <em>Fall 2021<strong>(head TA)</strong></em></li>
        <li>SML 310: Research Projects in Data Science, <em>Spring 2021<strong>(head TA)</strong></em></li>
        <li>ECE 201: Information Signals, <em>Spring 2020, Spring 2023</em></li>
    </ul>
    </li>

    
</ul>

</body>
</html>
