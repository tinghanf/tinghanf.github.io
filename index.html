<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="RaPdlErOpwDKyAdw8zZKzv_ciBUPEzwJUrAxWl7MWHM" />
    <title>Ting-Han Fan</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
        }
        #container {
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0px 0px 10px 0px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        #container h1 {
            text-align: center;
            color: #333;
        }
        #container img {
            display: block;
            margin: 0 auto;
        }
        #container p {
            text-align: justify;
            color: #666;
        }
        #container p a {
            text-decoration: none;
            color: #007bff;
        }
        #container ul {
            list-style-type: disc;
            padding: 1;
        }
        #container ul ul {
            margin-top: 5px;
        }
        #container ul li {
            margin-bottom: 10px;
            color: #666;
        }
        #container ul li a {
            text-decoration: none;
            color: #007bff;
        }
        .publication {
            border-bottom: 1px solid #eee;
            padding-bottom: 15px;
            margin-bottom: 20px;
        }
        .publication:last-child {
            border-bottom: none;
            margin-bottom: 0;
        }
    </style>
</head>
<body>

<div id="container">
    <center>
    <h1> Ting-Han Fan </h1>
    </center>
      
    <table border="0" cellpadding="5" cellspacing="10">
    <tbody><tr>

    <td valign="middle">
    <img src="fan-tinghan-01.jpg" height="200">
    </td>

    <td valign="middle">
    <p>I am a Research Scientist at the LLM reasoning and planning team of <a href="https://www.bytedance.com/en/">ByteDance</a>. Previously, I was 
    a machine learning engineer at <a href="https://shop.tiktok.com">TikTok E-commerce Recommendation</a>, 
    a summer associate at <a href="https://www.gsam.com">Goldman Sachs Asset Management</a>, 
    a research intern at <a href="https://www.siemens.com">Siemens</a> and <a href="https://www.microsoft.com">Microsoft</a>.
    I completed my M.A. and Ph.D. degrees at <a href="https://ece.princeton.edu/">Electrical and Computer Engineering, Princeton University</a>, where I was 
    very fortunate to be advised by Prof. <a href="https://ece.princeton.edu/people/peter-j-ramadge">Peter J. Ramadge</a>. 
    Before Princeton, I completed my B.S. degree at <a href="https://web.ee.ntu.edu.tw/">Electrical Engineering, National Taiwan University</a>.</p>

    <p>email: tinghanfan at gmail dot com</p>

    <p> Find me at <a href="https://scholar.google.com/citations?user=1mQ3kTEAAAAJ&hl=en">Google Scholar</a> 
    and <a href="https://www.linkedin.com/in/ting-han-fan">LinkedIn</a>. </p>
    </td>

    </tr></tbody>
    </table>
    <br>
    <br>

    <section>
        <h2>Publications:</h2>
        <div class="publication">
            <h4>Length Extrapolation of Transformers</h4>
            <ul>
                <li> <a href="https://aclanthology.org/2024.findings-naacl.10">Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, <strong>Findings of NAACL 2024.</strong></li>
                <li> <a href="https://aclanthology.org/2023.acl-long.756">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023. <a style="color:red">(Outstanding Paper Award)</a></strong> <a href="https://2023.aclweb.org/program/best_papers/">[link]</a></li>
                <li> <a href="https://aclanthology.org/2023.acl-short.102">Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings</a>, Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ACL 2023. <a style="color:blue">(Honorable Mention)</a></strong> <a href="https://2023.aclweb.org/program/best_papers/">[link]</a></li>
                <li> <a href="https://papers.nips.cc/paper_files/paper/2022/hash/37a413841a614b5414b333585e7613b8-Abstract-Conference.html">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation</a>, Ta-Chung Chi*, Ting-Han Fan*, Peter J. Ramadge, Alexander I. Rudnicky, <strong>NeurIPS 2022.</strong> <a href="https://github.com/chijames/KERPLE">[code]</a></li>
            </ul>
            
            <h4>Regular Language Reasoning</h4>
            <ul>
                <li><a href="https://aclanthology.org/2024.naacl-short.4">Advancing Regular Language Reasoning in Linear Recurrent Neural Networks</a>, Ting-Han Fan*, Ta-Chung Chi*, Alexander I. Rudnicky, <strong>NAACL 2024.</strong> <a href="https://github.com/tinghanf/RegluarLRNN">[code]</a></li>
                <li><a href="https://aclanthology.org/2023.findings-emnlp.397">Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation</a>, Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky, Peter J. Ramadge, <strong>Findings of EMNLP 2023.</strong></li>
            </ul>
        
            <h4>Reparameterization for Discrete Deep Generative Models</h4>
            <ul>
                <li> <a href="https://proceedings.mlr.press/v162/fan22a.html">Training Discrete Deep Generative Models via Gapped Straight-Through Estimator</a>, Ting-Han Fan*, Ta-Chung Chi*, Alexander I. Rudnicky, Peter J. Ramadge, <strong>ICML 2022.</strong> <a href="https://github.com/chijames/GST">[code]</a> <a href="https://slideslive.com/38984237/training-discrete-deep-generative-models-via-gapped-straightthrough-estimator">[SlidesLive]</a></li>
            </ul>
        
            <h4>Reinforcement Learning for Power Distribution System Controls</h4>
            <ul>
                <li> <a href="https://proceedings.mlr.press/v168/fan22a.html">PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems</a>, Ting-Han Fan, Xian Yeow Lee, Yubo Wang, <strong>L4DC 2022.</strong> <a href="https://github.com/siemens/powergym">[code]</a></li>
                <li> <a href="https://ieeexplore.ieee.org/document/9867395">Soft Actor-critic With Integer Actions</a>, Ting-Han Fan, Yubo Wang, <strong>ACC 2022.</strong></li>
            </ul>
            
            <h4>Model-based Reinforcement Learning</h4>
            <ul>
                <li> <a href="https://proceedings.mlr.press/v130/fan21a.html">A Contraction Approach to Model-based Reinforcement Learning</a>, Ting-Han Fan, Peter J. Ramadge, <strong>AISTATS 2021.</strong> <a href="https://slideslive.com/38952954/a-contraction-approach-to-modelbased-reinforcement-learning">[SildesLive]</a></li>
            </ul>
            <p>(* denotes equal contribution)</p>
        </div>
        <!-- More publication sections here -->
    </section>
    <br>

    <section>
        <h2>Patent Applications:</h2>
        <div class="patent applications">
            <ul>
                <li> <a href="https://patents.google.com/patent/US20230071450A1/en">System and method for controlling large scale power distribution systems using reinforcement learning</a>, Ting-Han Fan, Yubo Wang, Ulrich Muenz, <strong>US Patent App. 17/814535.</strong></li>
                <li> <a href="https://patents.google.com/patent/WO2023043601A1/en">System and method for supporting execution of batch production using reinforcement learning</a>, Ting-Han Fan, Yubo Wang, Ulrich Muenz, Mathias Hakenberg, <strong>WO Patent App. PCT/US2022/041711.</strong></li> 
            </ul>
        </div>
    </section>
    <br>
    <br>
    
    
    <section>
        <h2>Academic Activities:</h2>
        <div class="academic activities">
            <h4>Reviewer</h4>
            <ul>
                <li>NeurIPS 2022-2024, ICLR 2024, ICML 2022-2024, ISIT 2024, L4DC 2023, AISTATS 2021</li>
                <li><a href="https://aclrollingreview.org">ACL Rolling Review</a>: 2023 December - 2024 June</li>
            </ul>
            </li>
            
            <h4>Teaching Assistant at Princeton University</h4>
            <ul>
                <li>ECE 435/535: Machine Learning and Pattern Recognition, <em>Fall 2019, Fall 2020, Fall 2022 <strong>(head TA)</strong></em></li>
                <li>EGR 154: Foundations of Engineering: Linear Systems, <em>Spring 2022</em></li>
                <li>COS 302: Mathematics for Numerical Computing and Machine Learning, <em>Fall 2021 <strong>(head TA)</strong></em></li>
                <li>SML 310: Research Projects in Data Science, <em>Spring 2021 <strong>(head TA)</strong></em></li>
                <li>ECE 201: Information Signals, <em>Spring 2020, Spring 2023</em></li>
            </ul>
        </div>
    </section>

    <!-- More sections here -->
    <!--
    <footer>
        <p>&copy; 2024 Ting-Han Fan. All rights reserved.</p>
    </footer>
    -->
</div>

</body>
</html>
